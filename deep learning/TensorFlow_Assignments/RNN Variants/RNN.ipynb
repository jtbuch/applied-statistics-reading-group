{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Recurrent Neural Net (RNN) Lab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this lab, we will learn about recurrent neural nets, or rnns.\n",
    "\n",
    "First off, why whould you want to use an rnn? What do they look like and why are they better than what we have been using?\n",
    "\n",
    "Well, in the neural nets we have been using, our input must be a fixed size. This is okay for some problems, but is restrictive for other. For example, in the trigram language model you just implemented, we can predict the next word given the 2 previous words. Thats not really how we humans generate sentences. (We don't have amnesia after every third word.) What if we want to give the net 2 previous wourds one time, and 20 previous words another time?\n",
    "\n",
    "Enter the rnn. The rnn is used for inputs that are arranged in some order (a sequence). The input, for example, could be a stream of video, where you enter one frame at a time. Lets say our stream of video is a video of someone expressing a few emotions. The net could tell you something about each frame, (like a probability distribution over the emotions at that frame), or it could tell you something about the whole video (like a probability that the entire video was positive or negative). Your rnn is capable of outputting something at every timestep for you to use and base your loss on, or you could ignore all these potential outputs and just use the one at the end to calculate your loss to analyze the stream of data overall.\n",
    "\n",
    "Using an rnn to do our language modelling means that any given word can be conditionalized on (/ take into account) the whole past history of all the words that it has seen. This sounds much better right?\n",
    "\n",
    "So what does it look like..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](rnn.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Above is a diagram of the general idea. We have input x_t at timestep t, and this generates some output h_t. However, the rnn also sends its output back into itself to be taken into account for the next timestep. so every timestep is based on new information and also some previous information.\n",
    "\n",
    "\n",
    "Note that this presents a problem for starting your rnn at x_0. How do we can arround this? We define some intital starting state. (Generally this is all 0's, but it can be anything. It can even be learned via gradient descent.)\n",
    "\n",
    "\n",
    "So, how are these trained? Well, to visualize this, it is helpful to \"unroll\" the neural net. That is, visualize the rnn as 10 identical rnns..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN-unrolled.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, you could imagine running your net for 10 timesteps (with outputs h_0 to h_9) and then doing your backprop. First, lets ignore h_0-h_8 and just look at h_9. We could pretty esily now calculate the derivate of the loss (based on h_9) with repect the the paramters of A_9. (We know all the inputs and we know the activations and we know the loss function.) Once we have this, we could calculate the derivates of our loss for A_8, A_7, and so on. The gradient for h_9 flows from the right to the left of this unrolling. We call this \"backpropagation through time\". \n",
    "\n",
    "Ok, so now that we have the derivates in terms of the parameters in A_0 to A_9, what do we do to update the parameters of our real rnn, A?  Well, these are actually the same parameters... so... we add all of our changes together and take the average! This gets us our updates to the parameters of A.\n",
    "\n",
    "Now that we have our derivatives for the loss defined in terms of h_9, we could stop there. That is, we could ignore h_0 to h_8. But if we want to learn an output for each timestep, we should NOT stop here. In fact, what we should do is define our loss to be the sum of all the losses for h_0 to h_9. Because derivation distributes over addition, we could think of this as calulating the losses individually then adding them. That is, we move on to h_2, calculate our updates to A (based again on the average of those to A_0 to A_8), and add this to the updates suggested by h_9. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ok, do what do the internals of these rnns actually look like? \n",
    "\n",
    "The most basic implementaion is the \"Vanilla RNN\" (depicted bellow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](-1.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So what is going on here?\n",
    "\n",
    "First, note that there is a \"current output\" at the top, and a \"current state\" at the right. \n",
    "\n",
    "We have been talking about our rnn outputing h_t, and using this as our answer for timestep t and also feeding it into the next timestep to the right.\n",
    "\n",
    "However, the output we want at each timestep may be very low in dimensionality (let's call this y_t) and we want our rnn to be working with representations in higher dimensions. (For examle y_t,  could be 1 number 0-1 representing a probability, but we dont want to confine our rnn to be working in 1 dimensions and only passing one dimension of information to the next timestep).\n",
    "\n",
    "Because y_t often has low dimensionality, we actually do some processing to h_t to get y_t. This could be anything like a fully comnnected layer or any other type of layer you could imagine.\n",
    "\n",
    "It might be a good time to mention now that h_t stands for hidden state at timestep t. The state is a representation of all previous inputs and history.\n",
    "\n",
    "Okay, so how is the state h_t that is output (both into y_t and the next rnn at the next timestep), actually calculated?\n",
    "\n",
    "Next, we should note the four little squggly \"s\"s. There are 4 of them, which means that our internal state is a vector of size 4. These stand for a sigmoid operation applied to a vector of size 4. (Remember, signoid is an activation function that squishes numbers between 0 and 1). Ok, so how do we fill in this vector of size 4 before we apply the sigmoid? \n",
    "\n",
    "We can actually think about this in two seperate ways. First you could do one right matrix multiplicatoin on the prior state coming in, and a seperate one on the current input coming in. Both of these multiplications should shange the dimension of the of the vector to be 4. Then we can just add them.\n",
    "\n",
    "However, you could accomplish this is Tensorflow by just concatenating the the two vectors, and doing one matrix multiplication.\n",
    "\n",
    "After you do the multiplication (and additition if you are doing two matrix multiplications), you should add a bias term, then apply your sigmoid, and you are all done!!!\n",
    "\n",
    "Note... the bias is shown as a single \"1\", becuase you could think of concatenating a \"1\" to your input vector, then just adding another row to your matrix wiehgts to get these 1's to be whatever constant bias you want added to each new spot in the vector. (If you dont see this, don't worry about it. Its not important, but it is used in other areas of math and graphics so that you can multiply by a matrix and use that to also add a constant!)\n",
    "\n",
    "Heres the Vanilla RNN in math notation bellow...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](0.png)\n",
    "![title](1.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So how do we implement this? Well first, we dont actually do all of the backprop. we only go back like 20 timesteps from any one loss. The number of timesteps you go back is called the \"window size\". You can still have information encoded in your state from before that, but the loss just won't change your parameters more than window size steps back. This is called \"truncated back propagation\". You would take the loss from one out put, backprop it 20 steps back, then go on to your next one. \n",
    "\n",
    "Here's an image of whats going on, if you had a window size of 3..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](7.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "But this actually isnt quite how its done in Tensorflow. In Tensorflow, you generally pass in non-overlapping windows (stride by window size), and then just do your back prop of loss to the start of the window. That is if your window size is 20, the last element in your winodw will have its loss backpropped through all 20 timesteps, but the second to last one, will only have it back propped 19, and the 3rd to last will have 18 timesteps of backprop, and so on. Here is an example with window size of 3.\n",
    "\n",
    "Here's a pic..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](6.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can see that the 1st element in the window only has its loss backpropogated through that 1 time step, and the second loss only gets propagated through the first 2. Only the 3rd has the full backprop through 3 steps.\n",
    "\n",
    "Note: you could actually could do this Tensorflow-style backprop with overlapping windows, but then it makes it difficult to pass the correct state to the next timestep. (If you were striding by s, you would have to grab the state from timestep s and pass it in as your intial state to your next window. Most things in Tensorflow do not make that easy to do. To get around this you could just not pass the state between windows. (This is called a \"stateless\" model, as opposed to a \"stateful\" model.) However, then you would be capping the distance that information can be passed forward at your window size (in addition to already truncating your backprop.)\n",
    "\n",
    "However, this does make our batch forming a bit harder. If you have batch size \"B\", you actually need to split your data into batch into B sequences, so that you can do computation in parrallel.\n",
    "Let's imagine that your window size was 4 and your batch size was 2. You would split your data into 2 continuous sequences. Call these \"a\" and \"b\". (If this were a corpus of words, you would divide it in half.) Then your first batch should be:\n",
    "[[a_0, a_1, a_2, a_3],\n",
    " [b_0, b_1, b_2, b_3]]\n",
    "and the batch after that would be...\n",
    "[[a_4, a_5, a_6, a_7],\n",
    " [b_4, b_5, b_6, b_7]]\n",
    "This way all of the states calculated at the end of the first batch can be passed in to start youe next batch."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note on perplexity: We are printing perplexity here (e^avg loss). Recall that first we calculate our loss (which is cross-entropy), then perplexity is just the number of samples in a uniform distribution with that same entropy. (So it is \"as if\" you are chosing eaqually likely from that number of samples samples). \n",
    "Knowing this, we can see why wer raise e to the power of our loss: If we have a uniform distribution with N samples, the entropy is just N*((1/N)*-log_2(1/N)) = N*((1/N)*log_2(N)) = *log_2(N)... so now, if we wanted to recover N gien the entropy, we would just do 2^entropy = 2^log_2(N) = N... However we did our entropy loss in base \"e\", so we need to do all of this base e...\n",
    "\n",
    "Also note: we are printing perplexity for each batch. If you want the perplxity over the entire set of data, you would have to take the geometric mean over all batches (becuase of how powers work)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "please study this code and then walk your TA through it, to make sure you really know what going on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/jake/Desktop/model_files/model-1\n",
      "\n",
      "step 0 out of 4\n",
      "already done\n",
      "\n",
      "step 1 out of 4\n",
      "loss - 7.66942e+16\n",
      "perplexity - inf\n",
      "accuracy - 0.0255\n",
      "\n",
      "step 2 out of 4\n",
      "loss - inf\n",
      "perplexity - inf\n",
      "accuracy - 0.0295\n",
      "\n",
      "step 3 out of 4\n",
      "loss - nan\n",
      "perplexity - nan\n",
      "accuracy - 0.0665\n",
      "\n",
      "step 4 out of 4\n",
      "loss - nan\n",
      "perplexity - nan\n",
      "accuracy - 0.071\n"
     ]
    }
   ],
   "source": [
    "import glob, os, string, collections, math\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#supress annoying wanring\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "tf.reset_default_graph() #clears graph from last run (if you want to run again)... \n",
    "                         #HOWEVER, it pick up training where it left off, unless you delete the \"tensorboard_logs\" dir\n",
    "                         #And the model_files dir (in your current working directory)\n",
    "\n",
    "WINDOW_SZ = 20\n",
    "BATCH_SZ = 100\n",
    "EMBED_SZ = 30\n",
    "LEARN_RATE = 1e-4\n",
    "VOCAB_SZ = 8000\n",
    "SAVE_FREQ = 10 #save every 10 steps\n",
    "#only one epoch, so no epoch var\n",
    "\n",
    "BATCH_CAP = 5 #cap number of bathches so lab doesnt take too long\n",
    "\n",
    "#for simplicity, the size of our state will be the size of the y we want out.\n",
    "STATE_SZ = VOCAB_SZ\n",
    "\n",
    "MODEL_DIR = os.path.join(os.getcwd(), \"model_files\")\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"model\")\n",
    "SUMMARY_DIR = os.path.join(os.getcwd(), \"tensorboard_logs\")\n",
    "CORPUS_PATH = os.path.join(os.getcwd(), \"corpus.txt\") #define path str. (works on any OS)\n",
    "\n",
    "\n",
    "class VanillaRNN:\n",
    "    def __init__(self):\n",
    "        # define session and grpah\n",
    "        self.sess = tf.Session()\n",
    "        self.defineGraph()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        #check if the model exists already\n",
    "        checkPoint = tf.train.get_checkpoint_state(MODEL_DIR)\n",
    "        modelExists = checkPoint and checkPoint.model_checkpoint_path\n",
    "\n",
    "        #if it exists, load weights. Otherwise, init all weights.\n",
    "        if modelExists:\n",
    "            self.saver.restore(self.sess, checkPoint.model_checkpoint_path)\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def defineGraph(self):\n",
    "        #for plotting / visualization\n",
    "        self.train_writer = tf.summary.FileWriter(SUMMARY_DIR, self.sess.graph)\n",
    "\n",
    "        #keep a variable representing the step we are on. make sure it is not trainable.\n",
    "        self.glob_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "        self.initializer = tf.random_normal_initializer(stddev=0.1) #init vars from a noraml distribution\n",
    "\n",
    "        self.batch_in = tf.placeholder(tf.int64, shape=[BATCH_SZ, WINDOW_SZ]) \n",
    "\n",
    "        self.prev_state = tf.placeholder(tf.float32, shape=[BATCH_SZ, STATE_SZ]) \n",
    "        \n",
    "        self.embeddings = tf.Variable(tf.random_uniform([VOCAB_SZ, EMBED_SZ],-1.0,1.0))\n",
    "\n",
    "        #should have shape=[BATCH_SZ, WINDOW_SZ, EMBED_SZ]\n",
    "        self.embedded_batch_in = tf.nn.embedding_lookup(self.embeddings, self.batch_in)\n",
    "        \n",
    "        self.labels = tf.placeholder(tf.int32, shape=[BATCH_SZ, WINDOW_SZ])\n",
    "\n",
    "\n",
    "        # variables bellow are not stored in \"self\" because you will never need to ask sess for them. very hidden states.\n",
    "        # use \"get vairiable\" so i can give them names\n",
    "\n",
    "\n",
    "        #weights need to go from [x_t + s_t-1] to vector of size [STATE_SZ]\n",
    "        weights = tf.get_variable(\"W_IN\", dtype=tf.float32, shape=[EMBED_SZ+STATE_SZ, STATE_SZ], initializer=self.initializer)\n",
    "        biases = tf.get_variable(\"B_IN\", dtype=tf.float32, shape=[STATE_SZ], initializer=self.initializer)\n",
    "\n",
    "\n",
    "        #define recurrent part of network. If you use Tensorflows LSTM later on, this for loop will be done for you\n",
    "        states = [self.prev_state] #a list of state tensors\n",
    "        for i in xrange(WINDOW_SZ):   \n",
    "            curr_batch_input = self.embedded_batch_in[:,i,:] #get the input for time step i across all batches\n",
    "            #concatenate with previous state along row axis\n",
    "            cur_state = states[-1]\n",
    "            concat_last_state_and_input = tf.concat([cur_state, curr_batch_input], 1)\n",
    "\n",
    "            new_state = tf.matmul(concat_last_state_and_input, weights) + biases\n",
    "            states.append(new_state)\n",
    "\n",
    "        #out_states should have shape [batch_sz, \"window_sz\", state_sz] = [batch_sz, window_sz, vocab_sz] in our case\n",
    "        self.out_states = tf.stack(states[1:], axis=1) #turn list of tensors into one tensor. ignore first state fed in.\n",
    "\n",
    "        #soft max along our \"window_sz\" dimension\n",
    "        self.y_probabilities = tf.nn.softmax(self.out_states, dim=1)\n",
    "\n",
    "        #loss is average cross entropy across all words at all timesetos\n",
    "        self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.out_states, labels=self.labels))\n",
    "        \n",
    "        #keep a log of our loss function so we can visualize with Tensoarboard\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "\n",
    "        self.trainOp = tf.train.AdamOptimizer(learning_rate=LEARN_RATE).minimize(self.loss, global_step=self.glob_step)\n",
    "\n",
    "        #necessary step to save all summary logs for visualize with Tensoarboard\n",
    "        self.mergedSummaries = tf.summary.merge_all()\n",
    "\n",
    "    def save(self):\n",
    "        self.saver.save(self.sess, MODEL_PATH, global_step = self.glob_step)\n",
    "\n",
    "    def train(self, batchesData, batchesLabels):\n",
    "        previousState = np.zeros([BATCH_SZ, STATE_SZ]) #one state for each batch\n",
    "\n",
    "        for step in xrange(len(batchesData)):\n",
    "            print \"\\nstep\", step, \"out of\", len(batchesData)-1\n",
    "\n",
    "            #not yet caught up to where model has already trained:\n",
    "            if step < tf.train.global_step(self.sess, self.glob_step):\n",
    "                print \"already done\"\n",
    "                continue\n",
    "\n",
    "            batchData = batchesData[step]\n",
    "            batchLabels = batchesLabels[step]\n",
    "\n",
    "\n",
    "            feedDict = {self.batch_in: batchData, self.labels: batchLabels, self.prev_state: previousState}\n",
    "            sessArgs = [self.out_states, self.y_probabilities, self.loss, self.mergedSummaries, self.trainOp]\n",
    "\n",
    "            # RUN #\n",
    "            hiddenStates, probs, lossReturned, summary, _ = self.sess.run(sessArgs, feed_dict=feedDict)\n",
    "\n",
    "            #we are striding by window size, so use last state for next time\n",
    "            previousState = hiddenStates[:,-1,:] #take all states across batches from last step in window\n",
    "\n",
    "            print \"loss -\", lossReturned\n",
    "\n",
    "            predictedLabels = np.argmax(probs, axis=2)\n",
    "            numCorrect = np.sum(predictedLabels == batchLabels)\n",
    "            numInBatch = np.prod(np.array(batchLabels).shape)\n",
    "\n",
    "            print \"perplexity -\", math.e**lossReturned\n",
    "\n",
    "            print \"accuracy -\", float(numCorrect)/float(numInBatch)\n",
    "\n",
    "            #for visualizatoin. write your summary logs. \n",
    "            self.train_writer.add_summary(summary, step) \n",
    "\n",
    "            #save your model every once and a while\n",
    "            if step%SAVE_FREQ==0:\n",
    "                self.save()\n",
    "\n",
    "\n",
    "\n",
    "#helper func:\n",
    "\n",
    "#return same string with only letters left\n",
    "def filterOnlyLetters(strIn):\n",
    "    letterSet=set(string.letters)\n",
    "    return filter(lambda char: char in letterSet, strIn)\n",
    "\n",
    "\n",
    "def tokenizer(strIn):\n",
    "    \"\"\"\n",
    "    splits a string into lowercase words with only letters (no punctoination or numbers)\n",
    "    Args: \n",
    "        strIn (string): the sentence\n",
    "    Returns:\n",
    "        (list of str): a list of the words wihout punctination and all lower case\n",
    "\n",
    "    \"\"\"\n",
    "    spaceSeperatedFragments = strIn.strip().split() #seperate on white space\n",
    "    words = map(filterOnlyLetters, spaceSeperatedFragments) #remove punctuation and numbers\n",
    "    words = filter(lambda word: word!=\"\", words) #filter empty words\n",
    "    wordsLowerCase = [w.lower() for w in words]\n",
    "\n",
    "    return wordsLowerCase\n",
    "\n",
    "\n",
    "#main. read in data then train\n",
    "if __name__ == \"__main__\":\n",
    "    #read in data into batches then train...\n",
    "    \n",
    "\n",
    "    #read in corpus as words then process to number IDs.\n",
    "    corpusWords = [] #turn corpus into a list of words\n",
    "    \n",
    "    with open(CORPUS_PATH, \"r\") as corpusFile:\n",
    "        lines = corpusFile.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        corpusWords.extend(tokenizer(line))\n",
    "\n",
    "    #enforce VOCAB_SZ only keeping VOCAB_SZ-1 most common words. \n",
    "    #(We will add the word *UNK* to replace all of these)\n",
    "    wordCounts = collections.Counter(corpusWords)\n",
    "    commonWordCountTuples = wordCounts.most_common(VOCAB_SZ-1)\n",
    "    commonWords = zip(*commonWordCountTuples)[0]\n",
    "    allowedWords = set(list(commonWords)+[\"*UNK*\"])\n",
    "\n",
    "    corpusWordIDs = [] #corpus of numbers representing words\n",
    "    wordToIDs = {}\n",
    "    nextIDToUse = 0\n",
    "\n",
    "    for word in corpusWords:\n",
    "        word = word if word in allowedWords else \"*UNK*\" #if not allowed, make \"*UNK*\"\n",
    "        if word not in wordToIDs: #if word not in our dictionary, buy it valid, give it ID\n",
    "            wordToIDs[word] = nextIDToUse\n",
    "            nextIDToUse += 1\n",
    "        corpusWordIDs.append(wordToIDs[word]) #record next ID\n",
    "\n",
    "\n",
    "\n",
    "    # #split corpus Ids into windows\n",
    "    windowData = []\n",
    "    windowLabels = []\n",
    "    for i in xrange(0, len(corpusWordIDs)-WINDOW_SZ-1, WINDOW_SZ):\n",
    "        data = corpusWordIDs[i : i+WINDOW_SZ]\n",
    "        labels = corpusWordIDs[i+1 : i+WINDOW_SZ+1]\n",
    "        windowData.append(data)\n",
    "        windowLabels.append(labels)\n",
    "\n",
    "    #split windows into batches that opperate in parallel\n",
    "    batchesData = []\n",
    "    batchesLabels = []\n",
    "    numBatches = len(windowData)/BATCH_SZ # number of batches that will fit. also length of one sequence (in windows).\n",
    "    for batchIndex in xrange(0, numBatches):\n",
    "        batchData = [windowData[i*numBatches + batchIndex] for i in xrange(BATCH_SZ)]\n",
    "        batchLabel = [windowLabels[i*numBatches + batchIndex] for i in xrange(BATCH_SZ)]\n",
    "        batchesData.append(batchData)\n",
    "        batchesLabels.append(batchLabel)\n",
    "\n",
    "    #cap the number of batches so lab doesnt take took long\n",
    "    batchesData = batchesData[:BATCH_CAP]\n",
    "    batchesLabels = batchesLabels[:BATCH_CAP]\n",
    "\n",
    "\n",
    "    #INIT neural net\n",
    "    rnnModel = VanillaRNN()\n",
    "    #Train neural net\n",
    "    rnnModel.train(batchesData, batchesLabels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code bellow, we will run a command to visualize the training process using Tensorboard\n",
    "\n",
    "Please look at your terminal while it is running! There will be a port number. Enter into your web browser:\n",
    "\n",
    "\"http://localhost:xxxx\"\n",
    "\n",
    "where \"xxxxx\" is your port number.\n",
    "\n",
    "Then press The square stop symbol to terminate this server when done...\n",
    "\n",
    "Note, we are only visualizing training loss here. In real life, visualizing how your network performs on your validation (or development) data, as you train, is VERY useful. It doesnt matter that your training loss is getting better if your loss on your heldout data is not getting better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you see this, server has been terminated and you should have looked at Tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "logdir =os.path.join(os.getcwd(), MODEL_DIR) \n",
    "os.system(\"Tensorboard --logdir=\"+logdir) #command you would run in you terminal\n",
    "\n",
    "print \"if you see this, server has been terminated and you should have looked at Tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Off ^"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ok, now that we have mastered the Vanilla RNN, let's think about some problems with it.\n",
    "\n",
    "Do you guys remember the vanishing gradient problem? If we keep applying sigmoids (which have a derivative <1 everywhere and have a derivate that goes to 0 in the limit as out inputs get to be very big or very small), our gradient mass gets very small as we backpropagate to the begining of the network. When very positive or negative  activations then have a sigmoid applied, they are said to be saturated. (Batch normalization helps with this by keeping the inputs close to 0 and on either side. And this helps with the 0 derivate on the left side of ReLU too.)\n",
    "\n",
    "So repeated sigmoids application, and also repeated multiplications, cause a problem where derivatives get very small as your backpropagation goes farter and farther to the left.\n",
    "\n",
    "Becuase our Vanilla RNNS can have long inputs, it gets to be very hard for some early on input to have a derivative that is large enough to significantly impact a value later on down stream. The vanishing gradient problem becomes a real issue.\n",
    "\n",
    "so, how do we get around this???\n",
    "\n",
    "The answer is \"addition.\"\n",
    "\n",
    "Remember ResNet? By adding in skip connections, our gradient with repect to what we just added is \"1.\" This allows the gradient to flow backwards more easily.\n",
    "\n",
    "...Enter the Gated Recurrent Unit (GRU)...\n",
    "\n",
    "The GRU does not multiply the whole state every time, but instead protects the state by only adding or subtracting from it. How does it know what to add or subtract? Let's take a look..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](4.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Okay, so what's going on in this mess?\n",
    "\n",
    "First, note that at any juncture with a \"+\", the incoming vectors are just added together. At any juncture with a \"•\", the vectors are point wise multiplied.\n",
    "\n",
    "Okay, so you have the previous state (now S instead of h) coming in, along with your current intput.\n",
    "\n",
    "Let's look at \"r\" or the \"read gate\" (also refered to as the \"reset gate\"). the read gate determines what part of our previous state is important for current calculations (what we pay attention to). It \"reads\" the relevant parts. How does it do this? It takes a vector of numbers between 0 and 1 and pointwise multiplies them by the state, \"S_t-1\". (You can think of it as a mask). This tells it what fraction of each part of the state to read in. How does it calculate these numbers? Just a fully connected layer (one matrix multiplication) based on the concatenation of the last state S_t-1 and the current input X_t. (So this mask is learned.) That is, it learns what to read and it does this based on the the current input and what the last state actually was.\n",
    "\n",
    "Ok, now that we know the relevant info from the last state, we can propose a new state \"Ś\" (that is a squiggle on top). How do we propose this new state? By concatenating the vector that we just \"read in\" to the input (again), and doing another matrix multiplication. Except, to this resulting proposal state vector, we apply a Tanh activation (to squish the numbers between -1 and 1.) (All the square boxes are just a concatenation of the inputs followed by a matrix multiplicatoin then some activation function. You could also refer to them as \"sigmoid gates\" and \"tanh gates\".)\n",
    "\n",
    "Okay, so we have our new peoposal state Ś. Now, how to we combine this with our state S_t-1? We do a weighted combination of the two. That is, we take some vector of numbers 0-1, call it \"z\", and add z•S_t-1 to (1-z)•Ś. How do we calculate \"z\", our \"forget gate\"? you guessed it, another matrix multiplication applied to the concatenation of our previous state and current input.\n",
    "\n",
    "Now that we have added the weighted combination of Ś (our proposed state) and S_t-1 (our old state), we have S_t (our new state) and can output this and do it all over again!!!\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bellow is a network with an implmentation of a GRU... We will not actually run the network, but please walk your TA through it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRU\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph() #clears graph from last time\n",
    "\n",
    "INPUT_SZ = 2\n",
    "STATE_SZ = 3\n",
    "\n",
    "WINDOW_SZ = 20\n",
    "BATCH_SZ = 100\n",
    "\n",
    "batch_in = tf.placeholder(tf.float32, shape=[BATCH_SZ, WINDOW_SZ, INPUT_SZ]) \n",
    "prev_state = tf.placeholder(tf.float32, shape=[BATCH_SZ, STATE_SZ]) \n",
    "\n",
    "#wieghts for read and update. These are our masks.\n",
    "#### (why are these combined and not W_r and U_r independantly???)\n",
    "Weight_r = tf.Variable(initial_value=tf.random_normal(dtype=tf.float32, shape=[INPUT_SZ+STATE_SZ, STATE_SZ])) \n",
    "Weight_z = tf.Variable(initial_value=tf.random_normal(dtype=tf.float32, shape=[INPUT_SZ+STATE_SZ, STATE_SZ])) \n",
    "Bias_r = tf.Variable(initial_value=tf.random_normal(dtype=tf.float32, shape=[STATE_SZ])) \n",
    "Bias_z = tf.Variable(initial_value=tf.random_normal(dtype=tf.float32, shape=[STATE_SZ])) \n",
    "\n",
    "\n",
    "#weights for creating new state\n",
    "Weight_Proposal = tf.Variable(initial_value=tf.random_normal(dtype=tf.float32, shape=[INPUT_SZ+STATE_SZ, STATE_SZ])) \n",
    "Bias_Proposal = tf.Variable(initial_value=tf.random_normal(dtype=tf.float32, shape=[STATE_SZ])) \n",
    "\n",
    "\n",
    "#define recurrent part of network. If you use Tensorflows LSTM later on, this for loop will be done for you\n",
    "states = [prev_state] #a list of state tensors\n",
    "for i in xrange(WINDOW_SZ):   \n",
    "    curr_batch_input = batch_in[:,i,:] #get the input for time step i across all batches\n",
    "\n",
    "    #concatenate with previous state along row axis\n",
    "    cur_state = states[-1]\n",
    "    concat_last_state_and_input = tf.concat([cur_state, curr_batch_input], 1)\n",
    "\n",
    "    #now we have our masks\n",
    "    r = tf.sigmoid(tf.matmul(concat_last_state_and_input, Weight_r) + Bias_r)\n",
    "    z = tf.sigmoid(tf.matmul(concat_last_state_and_input, Weight_z) + Bias_z)\n",
    "\n",
    "    #\"*\"\" is element-wise\n",
    "    current_state_masked = cur_state*r\n",
    "    proposal_state_input = tf.concat([current_state_masked, curr_batch_input], 1)\n",
    "    proposal_state = tf.tanh(tf.matmul(proposal_state_input, Weight_Proposal) + Bias_Proposal)\n",
    "\n",
    "    new_state = cur_state*z + proposal_state*(1-z) #using \"1\" in tf will popilate a tensor of all 1's of whatever size you need\n",
    "    \n",
    "    states.append(new_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Off ^"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have learned about GRUs, let's take a look at one last type - the Long Short Term Memory (LSTM) unit.\n",
    "\n",
    "The LSTM is a bit more complicated, and so requires more training data, but in theory, can learn longer sequences. (GRUs are actually much newer; GRUs came out in 2014 and LSTMs came out in 1997.)\n",
    "\n",
    "Here is an LSTM deipcted bellow. We will go over its internals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](5.jpeg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So what is happening in this LSTM?\n",
    "\n",
    "Remember the pointwise multiplications regresented by \"•\"? Now they are represted by \"x\".\n",
    "\n",
    "Do you see the yellow sigmoid box and the tanh box? These are the sigmoid and tanh gates we used earlier for our GRU. They just represent a fully connected layer (one matrix multiplication) followed by the respective activiation.\n",
    "\n",
    "Do you see the light purple \"tanh\"? This is just an application of the tanh actiation to each neuron.\n",
    "\n",
    "See where lines merge? That is just a concatenation.\n",
    "\n",
    "Okay, now you should understand what the actual calculations mean. But why are we doing all this and what is intuitively going on?\n",
    "\n",
    "\n",
    "Well... think about it for a second? Can you figure it out?\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "Ok, thanks for humorring me...\n",
    "\n",
    "In our LSTM we now have two internal states. One is the hidden state h_t that will be later turned into y_t, and one (the top unlabled one) is the cell state, c_t. The cell state c_t maintains a richer encoding of your state and your history, and h_t is more like the information that is relevant to the calculation of y_t.\n",
    "\n",
    "The first yellow sigmoid gate on the left is for figuring how much of your previous c_t to remember and how much to forget.\n",
    "\n",
    "The yellow tanh gate is for generating your proposed state (ś in your GRU).\n",
    "\n",
    "the second yellow sigmoid gate is for figuring out which part of your proposed state to keep / focus on. (Then you add this to to the c_t you are calculating on the top line.)\n",
    "\n",
    "(Notice that in the GRU the new state is a weighted combination of your new proposed state and the old state, whereas in the LSTM, the parts of each to add together are learned seperately.)\n",
    "\n",
    "Finally, you output your c_t into the next time step. However, you also take a copy of this c_t, apply tanh activiation and your last sigmoid gate, and this h_t is what you deem is relevant to the output y_t. You pass this relevant infrmation (h_t) as the output and also as an input into your next timestep."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What more can you do with rnns?\n",
    "\n",
    "Well for our langauge model, you could use it to generate a senctence. How? You could pick a random start words (from a reasonable distribution) and then have the rnn predict the next word. Then you run it again, feeding in the word it just output (y_t) as the input to the next timestep (x_t+1)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We mentioned that you could add a fully connected layer to get h_t to be the desired overall output (y_t) that you want, based on your x_t. But you can actually add anything you want above the rnn layer. Or even before it! (You can process your input x_t into lower or higher dimensions before putting it in the rnn if you would think that would help.) \n",
    "\n",
    "In general, it only makes sense to have information going into the LSTM that you want to keep a history of. You could split off part of your x_y whose history does not matter and concatenate it with the h_t output of the rnn! \n",
    "\n",
    "You could even stack rnn layers on top of eachother!\n",
    "\n",
    "One cool think built with rnns is an \"attention model\". Basically, you split h_t so that in addition to outputting something that will be processed into y_t, it also outputs a mask for your next x_t, telling you where to focus next time! (A great application of this is captioning photos.)\n",
    "\n",
    "\n",
    "\n",
    "Tell your TA what you have learned and your ideas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Off ^"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
