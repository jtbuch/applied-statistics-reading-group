{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopfield Networks\n",
    "\n",
    "This will outline hopfield networks.\n",
    "\n",
    "As of now these notes will only go as far as we got in the reading group meeting on 3/23/19\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "<a href='#The task at hand'>The task at hand</a>\n",
    "\n",
    "<a href='#The Model System'>The Model System</a>\n",
    "\n",
    "<a href='#Updating States'>Updating States</a>\n",
    "\n",
    "<a href='#How to learn memories'>How to learn memories</a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a id='The task at hand'></a>\n",
    "\n",
    "#### The task at hand\n",
    "\n",
    "\n",
    "The task at hand is retrieving a memory from partial, potentially flawed information. This can be modeled by any system in which there is a flow in state space over time towards a set of locally stable attractors. Those attractors can represent memories and the initial conditions represent partial information. \n",
    "\n",
    "NOTE: ultimately I should insert some examples of such a system here: (need a system with multiple attractor states)\n",
    "    One example should be in one dimension over time, with a couple randomized initial conditions, the second should be a two dimensional vector field with at least two stable attractors that you can click anywhere on and have the path from that initial condition illustrate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is introduced in (Hopfield 1982) as a solution to a problem of finding a physical system that has \"general content-adressable memory\" with some error forgiveness. To the best I can gather (I was and remain largely unfamliar with the term), a content-adressable memory system provides a fast method of retrieving a memory or information packet from a lookup table. A a <i>general</i> content addressable memory system is described as a system that can retreive a full memory from partial information. \n",
    "\n",
    "A fun example is given in (Hopfield 1982) of a citation of a journal article. One might want a system that can recall a citation with partial information. Say the full citation is stored as: \n",
    "\n",
    "    Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proc Natl Acad Sci U S A 79, 2554â€“2558 (1982).\n",
    "\n",
    "A general content adressable memory system may be able to retrieve the memory with the string:\n",
    "\n",
    "    Hopfield (1982)\n",
    "\n",
    "And a system with error tolerance may be able to retrieve the string with:\n",
    "\n",
    "    Jumpfield (1982)\n",
    "\n",
    "The semantic example is both intended for humour and as illustrative. The system will be more likely to restore a memory that is closer in some way to the true memory.\n",
    "\n",
    "NOTE: Provided I have enough time (so this is unlikely to happen) I'll code the citations for this piece into a hopfield network and see how good the network is at retrieving citations if we input a string to try to recall them.\n",
    "\n",
    "<a id='The Model System'></a>\n",
    "\n",
    " \n",
    "\n",
    "#### The model system\n",
    "\n",
    "The system will be described by a set of nodes that can be represented by a state vector $\\vec{X}$ with N coordinates $x_1, x_2,...,x_N$ where N corresponds to the number of nodes in the network. There will be a set of locally stable memories corresponding to specific states $\\vec{X}_a, \\vec{X}_b,..., \\vec{X}_s$ where s corresponds to the number of input memories\n",
    "\n",
    "We want to be able to give an initial input $\\vec{X}_0 = \\vec{X}_a + \\Delta$ and have the system flow towards and stably land on $\\vec{X}_a$\n",
    "\n",
    "\n",
    "We'll call each node a neuron, which can have a value $V_i \\in \\{0,1\\}$ \n",
    "\n",
    "In Hopfields original formulation these values represented not firing and firing, though there are other formulations, such as that presented in (Hopfield 1983) where the state space is [-1,1] (maybe we'll get to that later). \n",
    "\n",
    "The connection weights between nodes are represented by a matrix $T_{ij}$\n",
    "\n",
    "Each node is connected to every <i>other</i> node (nodes are not connected to themselves); $T_{ii} =0$  $\\forall i$ \n",
    "\n",
    "In the original formulation, which we'll focus on here, the weights are symmetric, though one of the main thesis of the 1982 paper was that \"<i>The flow in phase space produced by this model algorithm has the properties nexessary for a phsyical content-addressable memory </i> whether or not $T_{ij}$ is symmetric\"\n",
    "\n",
    "(For our conversation for now) $T_{ij} = T_{ji}$\n",
    "\n",
    "<a id='Updating States'></a>\n",
    "\n",
    "\n",
    "\n",
    "#### Updating states \n",
    "\n",
    "The state is changed over time asynchronously, such that one randomly selected node $x_i$ changes it's state $V_i$ according to the states of the nodes connected to it and a predetermined threshold $U_i$\n",
    "\n",
    "$$\n",
    "V_i \\rightarrow 1  \\ \\  if \\ \\  \\sum_{j\\neq i} T_{ij}V_j > U_i\n",
    "$$\n",
    "$$\n",
    "V_i \\rightarrow 0  \\ \\  if \\ \\  \\sum_{j\\neq i} T_{ij}V_j < U_i\n",
    "$$\n",
    "\n",
    "By default $U_i = 0$ though this need not be the case (and in some cases changes the threshold can be useful)\n",
    "\n",
    "<a id='How to learn memories'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Initializing and updating weights (how to learn memories)\n",
    "\n",
    "A storage prescription is offered that, given the network and the updating algorythim, generates stable memories. Following along the notation of (Hopfield 1982) we'll represent the set of states we wish to store as $\\vec{V}^s, s=1...n$ \n",
    "\n",
    "The perscriptions is as follows:\n",
    "\n",
    "$$\n",
    "T_{ij} = \\sum_{s} (2v^s_i - 1)(2v^s_j - 1)\n",
    "$$\n",
    "$$\n",
    "T_{ii} = 0\n",
    "$$\n",
    "\n",
    "Justification of stability:\n",
    "\n",
    "NOTE: This justification is outlined in (Hopfield 1982) but I'm gonna expand on it a bit here to the best of my ability. In doing so I will introduce an additional assumption that is not explicit in the original paper. Generally speaking I would take this explanation with a grain of salt.\n",
    "\n",
    "We'll combine the above perscription with our updating mechanism, making an update from a current state we'll call $V^{s'}$ \n",
    "\n",
    "$$\n",
    "\\sum_j T_{ij} V_j^{s'} = \\sum_s (2V_i^s - 1)[\\sum_j V_j^{s'}(2V_j^s -1)]\n",
    "$$\n",
    "\n",
    "Let's look first at the value within the bracket:\n",
    "\n",
    "$$\n",
    "[\\sum_j V_j^{s'}(2V_j^s -1)]\n",
    "$$\n",
    "\n",
    "We'll examine the values the factors of the term in brackets can take as well as the values the full product can take.\n",
    "\n",
    "$$\n",
    "V_j^{s'} \\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(2V_j^s -1) \\in \\{-1,1\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "[\\sum_j V_j^{s'}(2V_j^s -1)] \\in \\{1,-1,0,0\\}\n",
    "$$\n",
    "\n",
    "! Assuming a roughly equal number of states at a given time and for each memory !\n",
    "\n",
    "If $V^s \\neq V^{s'}$ the mean value of the bracketed term will be 0\n",
    "\n",
    "If $V^s = V^{s'}$ the state space of the product will be reduced to $\\{1,0\\}$ and the mean value will be N/2\n",
    "\n",
    "Given this:\n",
    "\n",
    "$$\n",
    "\\sum_j T_{ij}V_j^{s'}  \\approx 0 \\ \\ if \\ \\ s\\neq  s^{'}\n",
    "$$\n",
    " \n",
    "$$\n",
    "\\sum_j T_{ij} V_j^{s'} \\approx (2V_i^{s'} -1)N/2 \\ \\ if\\ \\ s = s^{'} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if in a target memory state (and thus $s=s^{'}$) the sign of $\\sum_j T_{ij} V_j^{s'}$ is determined by the current state (and recall the threshold for choosing the state update is 0).\n",
    "\n",
    "$$ \n",
    "If V_i^{s'} = 1 \\rightarrow \\sum_j T_{ij} V_j^{s'} > 0\n",
    "$$\n",
    "$$\n",
    "If V_i^{s'} = 0 \\rightarrow \\sum_j T_{ij} V_j^{s'} < 0\n",
    "$$\n",
    "\n",
    "Stability! Insert celebration gif here (if possible)\n",
    "\n",
    "NOTE: It may be the case, now that I'm thinking about it again, that the assumption of roughly equivalent numbers of state values is unneccesarry. If thinking about the state spaces, regardless of the specific value of the secondary term it must <i>necessarally</i> be postive when $s = s^{'}$ (state space of {0,1} as opposed to {-1,0,1}). Thus with a threshold of 0 it doesn't matter for the stability of the target states that there be an equitable distrubtion of states, rather that condition may be more benificial for not creating spurious stable states. I have to think about this more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy\n",
    "\n",
    "To be added at later date\n",
    "\n",
    "#### Properties\n",
    "\n",
    "Both trained and opposite states will be stable, considered \"nominal\" in (Hopfield 1982).\n",
    "\n",
    "These states will be grossly preferred up until the point that s/N is around 0.15 after which an increase in failures will be observed\n",
    "\n",
    "The state of all zeros will be stable but energetically unfavored until the threshold is raised sufficiently, if the threshold is sufficiently raised this can be used as a \"not familiar state\" though there are observable statistics at the population level that can also indicate familiarity (speed of energy fall I think is one)\n",
    "\n",
    "Some states are preferred to others (if randomizing initial conditions).\n",
    "\n",
    "There was some debate in section as to the number of sampled initial conditions necessary to sample before posterior can be reasonably expected to mix. It was pointed out that the number of possible states is on the order of $2^N$ and that may reflect the order of necessary samples. I argue that need not be true (with much gusto though little confidence). While there is some degree of stochasticity in this system (introduced by updating states in a random order) the system can still be thought of a (noisy) vector field. The order of magnitude of samples necessary is linked to the degree to which there is local homogeneity in the gradient. The more homogeneity the fewer samples necessary before mixing. This in turn I believe is determined by how the memories are organized in the weight space. If memories are nicely spaced and separated and paths smoothly tend toward a particular place from a large region the posterior will stabilize quite quickly (empirically as quickly as $2^{10}$ samples for a system with 20 nodes), however if memories are spaced such as to create a more chaotic vector field it may take more the order of examples of >$2^N$\n",
    "\n",
    "By empirically seemigng mixed I mean that when looking at landings on nominal memory states the landing histogram shape stops changing (see code with randomized intializations).\n",
    "\n",
    "#### Spurious memories\n",
    "\n",
    "(Hopfield 1983) outlines some interesting cases of spurious memories.\n",
    "\n",
    "The semantic example given there will be repeated here:\n",
    "\n",
    "Memory 1:          Walter,white\n",
    "\n",
    "Memory 2:          Walter,black\n",
    "\n",
    "Memory 3:          Harold,grey\n",
    "\n",
    "Spurious Memory:          Walter,grey\n",
    "\n",
    "\"where grey is taken as a category equally esembling black and white\"\n",
    "In this situation Walter and grey have significiant correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources\n",
    "\n",
    "1.\tHopfield, J. J., Feinstein, D. I. & Palmer, R. G. â€˜Unlearningâ€™ has a stabilizing effect in collective memories. Nature 304, 158 (1983).\n",
    "2.\tHopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proc Natl Acad Sci U S A 79, 2554â€“2558 (1982).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
